{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b48df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize # Using scikit-image for resizing\n",
    "\n",
    "# Keras / TensorFlow Components\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense, Dropout,\n",
    "                                     GlobalMaxPooling1D, Lambda, concatenate)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, roc_curve\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aa5c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Plane: sagittal\n",
      "Model will be saved to: ./output_models/xception_mrnet_sagittal_best.h5\n"
     ]
    }
   ],
   "source": [
    "# TODO: Set these paths according to your MRNet dataset location\n",
    "BASE_DATA_DIR = './dataset/' # Base directory of the extracted MRNet dataset\n",
    "OUTPUT_DIR = './output_models/' # Where to save trained models and logs\n",
    "\n",
    "# Select the plane to train on\n",
    "# Options: 'axial', 'coronal', 'sagittal'\n",
    "PLANE = 'sagittal'\n",
    "\n",
    "# Model & Training Hyperparameters\n",
    "IMG_SIZE = (299, 299) # Input size for Xception\n",
    "N_CHANNELS = 3 # Xception expects 3 channels (we'll stack grayscale)\n",
    "BATCH_SIZE = 4 # <<< Adjust based on GPU memory (can be small like 2 or 4)\n",
    "EPOCHS = 50 # Max number of training epochs (EarlyStopping will likely stop it sooner)\n",
    "LEARNING_RATE = 1e-4 # Initial learning rate for Adam optimizer\n",
    "DROPOUT_RATE = 0.5 # Dropout rate for the classification head\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Construct model save path based on the chosen plane\n",
    "MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, f'xception_mrnet_{PLANE}_best.h5')\n",
    "print(f\"Selected Plane: {PLANE}\")\n",
    "print(f\"Model will be saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ab5524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs detected. Running on CPU (will be very slow).\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"Successfully configured memory growth for {len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(f\"Error setting memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPUs detected. Running on CPU (will be very slow).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f5527da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_dir, task, split):\n",
    "    \"\"\"Loads labels for a specific task (acl, meniscus) and split (train, valid).\"\"\"\n",
    "    label_path = os.path.join(label_dir, f\"{split}-{task}.csv\")\n",
    "    try:\n",
    "        labels_df = pd.read_csv(label_path, header=None, names=['exam_id', 'label'], dtype={'exam_id': str})\n",
    "        # Filter out rows with non-integer labels\n",
    "        labels_df = labels_df[pd.to_numeric(labels_df['label'], errors='coerce').notnull()]\n",
    "        labels_df['label'] = labels_df['label'].astype(int)  # Convert to integer\n",
    "        labels_df.set_index('exam_id', inplace=True)\n",
    "        print(f\"Loaded {len(labels_df)} labels for {task} - {split} split.\")\n",
    "        return labels_df['label'].to_dict()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Label file not found at {label_path}\")\n",
    "        return {}\n",
    "\n",
    "def preprocess_slice(slice_img, target_size=(299, 299)):\n",
    "    \"\"\"Preprocesses a single 2D slice: resizes, normalizes, and stacks to 3 channels.\"\"\"\n",
    "    # Ensure input is float for calculations\n",
    "    slice_img = slice_img.astype(np.float32)\n",
    "\n",
    "    # 1. Resize\n",
    "    # Note: anti_aliasing=True is generally recommended but can be slower.\n",
    "    # You might need `pip install scikit-image`\n",
    "    slice_resized = resize(slice_img, target_size, anti_aliasing=True, preserve_range=True) # preserve_range=True is important before custom norm\n",
    "\n",
    "    # 2. Normalize (Example: Scale to [0, 1] based on slice intensity)\n",
    "    # The MRNet paper used scan-specific Z-score normalization, which might be better.\n",
    "    # This simple scaling is easier to implement initially.\n",
    "    min_val = np.min(slice_resized)\n",
    "    max_val = np.max(slice_resized)\n",
    "    if max_val > min_val:\n",
    "        slice_normalized = (slice_resized - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        slice_normalized = np.zeros(target_size) # Handle blank slices\n",
    "\n",
    "    # 3. Stack to 3 Channels for Xception input\n",
    "    slice_3channel = np.stack([slice_normalized] * 3, axis=-1)\n",
    "\n",
    "    return slice_3channel.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75df18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRNetSequence(Sequence):\n",
    "    \"\"\" Keras Sequence for loading MRNet data slice by slice. \"\"\"\n",
    "    def __init__(self, data_dir, plane, labels_acl, labels_meniscus, exam_ids, batch_size, target_size, is_train=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.plane = plane\n",
    "        self.labels_acl = labels_acl\n",
    "        self.labels_meniscus = labels_meniscus\n",
    "        # Ensure exam_ids are strings to match dictionary keys\n",
    "        self.exam_ids = [str(eid) for eid in exam_ids]\n",
    "        self.batch_size = batch_size # Note: this is exam batch size\n",
    "        self.target_size = target_size\n",
    "        self.is_train = is_train # Flag to enable shuffling for training data\n",
    "        self.indices = np.arange(len(self.exam_ids))\n",
    "        if self.is_train:\n",
    "            self.shuffle() # Initial shuffle for training\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of batches per epoch (based on exams)\n",
    "        return int(np.ceil(len(self.exam_ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get batch indices for exams\n",
    "        batch_exam_indices = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        batch_exam_ids = [self.exam_ids[i] for i in batch_exam_indices]\n",
    "\n",
    "        batch_slices_list = []\n",
    "        batch_labels_acl_list = []\n",
    "        batch_labels_meniscus_list = []\n",
    "\n",
    "        for exam_id in batch_exam_ids:\n",
    "            exam_path = os.path.join(self.data_dir, self.plane, f\"{exam_id}.npy\")\n",
    "            try:\n",
    "                # Load the 3D volume for the exam\n",
    "                volume = np.load(exam_path) # Shape: (num_slices, height, width)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"\\nWarning: File not found {exam_path}. Skipping exam {exam_id}.\")\n",
    "                continue\n",
    "            except ValueError as e:\n",
    "                 print(f\"\\nWarning: Error loading {exam_path}: {e}. Skipping exam {exam_id}.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            # Get exam labels (using .get for safety, although IDs should exist)\n",
    "            label_acl = self.labels_acl.get(exam_id)\n",
    "            label_meniscus = self.labels_meniscus.get(exam_id)\n",
    "\n",
    "            if label_acl is None or label_meniscus is None:\n",
    "                print(f\"\\nWarning: Labels not found for exam {exam_id}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Preprocess each slice and collect\n",
    "            num_slices_in_volume = volume.shape[0]\n",
    "            for i in range(num_slices_in_volume):\n",
    "                slice_img = volume[i]\n",
    "                processed_slice = preprocess_slice(slice_img, self.target_size)\n",
    "                batch_slices_list.append(processed_slice)\n",
    "                batch_labels_acl_list.append(label_acl)\n",
    "                batch_labels_meniscus_list.append(label_meniscus)\n",
    "\n",
    "        # Convert lists to numpy arrays for the batch\n",
    "        batch_slices_np = np.array(batch_slices_list)\n",
    "        batch_labels_acl_np = np.array(batch_labels_acl_list, dtype=np.float32)\n",
    "        batch_labels_meniscus_np = np.array(batch_labels_meniscus_list, dtype=np.float32)\n",
    "\n",
    "        # Return batch data in the format Keras expects\n",
    "        # x: batch of slices, y: dictionary matching model output names\n",
    "        return batch_slices_np, {'acl_output': batch_labels_acl_np, 'meniscus_output': batch_labels_meniscus_np}\n",
    "\n",
    "    def shuffle(self):\n",
    "         \"\"\"Shuffles the order of exams\"\"\"\n",
    "         np.random.shuffle(self.indices)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle exam indices after each epoch if this is for training\n",
    "        if self.is_train:\n",
    "            self.shuffle()\n",
    "            # print(\"\\nShuffled training exam indices for next epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1a47937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xception_model(input_shape, num_classes_acl=1, num_classes_meniscus=1, dropout_rate=0.5):\n",
    "    \"\"\"Builds the Xception model for slice-level prediction with two output heads.\"\"\"\n",
    "    # --- Base Model (Xception) ---\n",
    "    # include_top=False: Remove the original ImageNet classification layer\n",
    "    # pooling=None: We'll add our own pooling after the base model\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape, pooling=None)\n",
    "\n",
    "    # --- Freeze Base Model Layers ---\n",
    "    # Start by freezing the pre-trained weights. We'll only train the new layers initially.\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # --- Input Layer ---\n",
    "    # Define the input tensor matching the preprocessed slice shape\n",
    "    slice_input = Input(shape=input_shape, name=\"slice_input\")\n",
    "\n",
    "    # --- Connect Input to Base Model ---\n",
    "    # Pass the input through the base model.\n",
    "    # training=False is crucial here when base_model is frozen, to ensure BatchNorm layers run in inference mode.\n",
    "    x = base_model(slice_input, training=False)\n",
    "\n",
    "    # --- Pooling Layer (Per Slice) ---\n",
    "    # Apply Global Average Pooling to the output features of the base model for each slice.\n",
    "    # This reduces spatial dimensions to a single feature vector per slice.\n",
    "    x = GlobalAveragePooling2D(name=\"slice_gap\")(x)\n",
    "\n",
    "    # --- Classification Head ---\n",
    "    # Add Dropout for regularization before the final Dense layers\n",
    "    x = Dropout(dropout_rate, name=\"head_dropout\")(x)\n",
    "\n",
    "    # Output layer for ACL Tear classification (Sigmoid for binary probability)\n",
    "    acl_output = Dense(num_classes_acl, activation='sigmoid', name='acl_output')(x)\n",
    "\n",
    "    # Output layer for Meniscus Tear classification (Sigmoid for binary probability)\n",
    "    # This head branches off from the same pooled & dropout features.\n",
    "    meniscus_output = Dense(num_classes_meniscus, activation='sigmoid', name='meniscus_output')(x)\n",
    "\n",
    "    # --- Create and Return Model ---\n",
    "    # Define the model with one input (slice_input) and two outputs (acl_output, meniscus_output)\n",
    "    model = Model(inputs=slice_input, outputs=[acl_output, meniscus_output], name=f\"Xception_MRNet_{PLANE}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b0d1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_aggregated(model, data_dir, plane, labels_acl, labels_meniscus, exam_ids, target_size):\n",
    "    \"\"\"Evaluates the slice-level model using exam-level aggregation (Max Pooling).\"\"\"\n",
    "    true_labels_acl = []\n",
    "    true_labels_meniscus = []\n",
    "    pred_probs_acl = []\n",
    "    pred_probs_meniscus = []\n",
    "\n",
    "    print(f\"\\nStarting exam-level evaluation on {len(exam_ids)} exams using plane '{plane}'...\")\n",
    "    progbar = tf.keras.utils.Progbar(len(exam_ids))\n",
    "\n",
    "    # Ensure exam_ids are strings\n",
    "    exam_ids = [str(eid) for eid in exam_ids]\n",
    "\n",
    "    for i, exam_id in enumerate(exam_ids):\n",
    "        exam_path = os.path.join(data_dir, plane, f\"{exam_id}.npy\")\n",
    "        try:\n",
    "            volume = np.load(exam_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Eval File not found {exam_path}. Skipping exam {exam_id}.\")\n",
    "            progbar.update(i + 1) # Update progress bar even if skipped\n",
    "            continue\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Error loading eval file {exam_path}: {e}. Skipping exam {exam_id}.\")\n",
    "            progbar.update(i + 1)\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Get true labels for the exam\n",
    "        label_acl = labels_acl.get(exam_id)\n",
    "        label_meniscus = labels_meniscus.get(exam_id)\n",
    "\n",
    "        if label_acl is None or label_meniscus is None:\n",
    "            print(f\"Warning: Eval Labels not found for exam {exam_id}. Skipping.\")\n",
    "            progbar.update(i + 1)\n",
    "            continue\n",
    "\n",
    "        # Process all slices in the volume\n",
    "        exam_slices_processed = []\n",
    "        num_slices_in_volume = volume.shape[0]\n",
    "        if num_slices_in_volume == 0:\n",
    "             print(f\"Warning: Exam {exam_id} has 0 slices. Skipping.\")\n",
    "             progbar.update(i + 1)\n",
    "             continue\n",
    "\n",
    "        for slice_idx in range(num_slices_in_volume):\n",
    "            processed_slice = preprocess_slice(volume[slice_idx], target_size)\n",
    "            exam_slices_processed.append(processed_slice)\n",
    "\n",
    "        # Stack slices into a batch for prediction\n",
    "        exam_slices_np = np.array(exam_slices_processed)\n",
    "\n",
    "        # Get slice-level predictions from the model\n",
    "        # model.predict yields a list [preds_acl, preds_meniscus]\n",
    "        # each element has shape (num_slices, 1)\n",
    "        slice_preds = model.predict(exam_slices_np, batch_size=len(exam_slices_np), verbose=0)\n",
    "        slice_preds_acl = slice_preds[0].flatten() # Shape: (num_slices,)\n",
    "        slice_preds_meniscus = slice_preds[1].flatten() # Shape: (num_slices,)\n",
    "\n",
    "        # --- Aggregation Step (Max Pooling) ---\n",
    "        # Take the maximum probability across all slices for this exam\n",
    "        exam_pred_prob_acl = np.max(slice_preds_acl) if len(slice_preds_acl) > 0 else 0.5 # Default to 0.5 if no slices\n",
    "        exam_pred_prob_meniscus = np.max(slice_preds_meniscus) if len(slice_preds_meniscus) > 0 else 0.5\n",
    "\n",
    "        # Store results\n",
    "        true_labels_acl.append(label_acl)\n",
    "        true_labels_meniscus.append(label_meniscus)\n",
    "        pred_probs_acl.append(exam_pred_prob_acl)\n",
    "        pred_probs_meniscus.append(exam_pred_prob_meniscus)\n",
    "\n",
    "        progbar.update(i + 1) # Update progress bar\n",
    "\n",
    "    # --- Calculate Metrics ---\n",
    "    print(\"\\n--- Aggregated Evaluation Results ---\")\n",
    "    if not true_labels_acl: # Check if any exams were successfully processed\n",
    "         print(\"No valid exams found or processed for evaluation.\")\n",
    "         return None, None # Return None for AUCs if no data\n",
    "\n",
    "    # Convert lists to numpy arrays for sklearn metrics\n",
    "    true_labels_acl = np.array(true_labels_acl)\n",
    "    true_labels_meniscus = np.array(true_labels_meniscus)\n",
    "    pred_probs_acl = np.array(pred_probs_acl)\n",
    "    pred_probs_meniscus = np.array(pred_probs_meniscus)\n",
    "\n",
    "    # Calculate predicted labels based on 0.5 threshold for Accuracy\n",
    "    pred_labels_acl = (pred_probs_acl > 0.5).astype(int)\n",
    "    pred_labels_meniscus = (pred_probs_meniscus > 0.5).astype(int)\n",
    "\n",
    "    # Calculate AUC, Accuracy, Log Loss for each task\n",
    "    auc_acl = roc_auc_score(true_labels_acl, pred_probs_acl)\n",
    "    acc_acl = accuracy_score(true_labels_acl, pred_labels_acl)\n",
    "    loss_acl = log_loss(true_labels_acl, pred_probs_acl)\n",
    "\n",
    "    auc_meniscus = roc_auc_score(true_labels_meniscus, pred_probs_meniscus)\n",
    "    acc_meniscus = accuracy_score(true_labels_meniscus, pred_labels_meniscus)\n",
    "    loss_meniscus = log_loss(true_labels_meniscus, pred_probs_meniscus)\n",
    "\n",
    "    # Calculate the average AUC (primary MRNet metric)\n",
    "    avg_auc = (auc_acl + auc_meniscus) / 2.0\n",
    "\n",
    "    print(f\"Results for Plane: {plane}\")\n",
    "    print(f\"ACL Task:\")\n",
    "    print(f\"  AUC:      {auc_acl:.4f}\")\n",
    "    print(f\"  Accuracy: {acc_acl:.4f} (Threshold 0.5)\")\n",
    "    print(f\"  Log Loss: {loss_acl:.4f}\")\n",
    "    print(f\"Meniscus Task:\")\n",
    "    print(f\"  AUC:      {auc_meniscus:.4f}\")\n",
    "    print(f\"  Accuracy: {acc_meniscus:.4f} (Threshold 0.5)\")\n",
    "    print(f\"  Log Loss: {loss_meniscus:.4f}\")\n",
    "    print(f\"-----------------------------\")\n",
    "    print(f\"Average Exam AUC: {avg_auc:.4f}\")\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "    # Return calculated metrics (optional)\n",
    "    results = {\n",
    "        'acl': {'auc': auc_acl, 'acc': acc_acl, 'loss': loss_acl, 'true': true_labels_acl, 'pred_prob': pred_probs_acl},\n",
    "        'meniscus': {'auc': auc_meniscus, 'acc': acc_meniscus, 'loss': loss_meniscus, 'true': true_labels_meniscus, 'pred_prob': pred_probs_meniscus},\n",
    "        'avg_auc': avg_auc\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d00cc9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels...\n",
      "Loaded 1130 labels for acl - train split.\n",
      "Loaded 1130 labels for meniscus - train split.\n",
      "Loaded 120 labels for acl - valid split.\n",
      "Loaded 120 labels for meniscus - valid split.\n",
      "\n",
      "Found 1130 training exams for plane sagittal.\n",
      "Found 120 validation exams for plane sagittal.\n",
      "\n",
      "Creating data generators...\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = os.path.join(BASE_DATA_DIR, 'train')\n",
    "valid_data_dir = os.path.join(BASE_DATA_DIR, 'valid')\n",
    "label_dir = BASE_DATA_DIR # Assuming label CSVs are in the base directory\n",
    "\n",
    "# --- Load Labels ---\n",
    "print(\"Loading labels...\")\n",
    "train_labels_acl = load_labels(label_dir, 'acl', 'train')\n",
    "train_labels_meniscus = load_labels(label_dir, 'meniscus', 'train')\n",
    "valid_labels_acl = load_labels(label_dir, 'acl', 'valid')\n",
    "valid_labels_meniscus = load_labels(label_dir, 'meniscus', 'valid')\n",
    "\n",
    "# --- Get Exam IDs ---\n",
    "# Assumes exam IDs are the filenames without the .npy extension\n",
    "# Use chosen PLANE to list files\n",
    "try:\n",
    "    train_exam_ids = sorted([f.split('.')[0] for f in os.listdir(os.path.join(train_data_dir, PLANE)) if f.endswith('.npy')])\n",
    "    valid_exam_ids = sorted([f.split('.')[0] for f in os.listdir(os.path.join(valid_data_dir, PLANE)) if f.endswith('.npy')])\n",
    "    print(f\"\\nFound {len(train_exam_ids)} training exams for plane {PLANE}.\")\n",
    "    print(f\"Found {len(valid_exam_ids)} validation exams for plane {PLANE}.\")\n",
    "    # Simple validation: Check if first few IDs exist in loaded labels\n",
    "    if train_exam_ids and (train_exam_ids[0] not in train_labels_acl or train_exam_ids[0] not in train_labels_meniscus):\n",
    "         print(f\"Warning: Mismatch detected between file IDs (e.g., {train_exam_ids[0]}) and label IDs. Check load_labels and file names.\")\n",
    "    if valid_exam_ids and (valid_exam_ids[0] not in valid_labels_acl or valid_exam_ids[0] not in valid_labels_meniscus):\n",
    "         print(f\"Warning: Mismatch detected between file IDs (e.g., {valid_exam_ids[0]}) and label IDs.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nError: Could not find data directory for plane '{PLANE}' in {train_data_dir} or {valid_data_dir}.\")\n",
    "    print(\"Please ensure BASE_DATA_DIR is correct and contains 'train'/'valid' subfolders with '{PLANE}' subfolders.\")\n",
    "    # Stop execution if data isn't found\n",
    "    raise\n",
    "\n",
    "# --- Create Data Generators ---\n",
    "print(\"\\nCreating data generators...\")\n",
    "train_gen = MRNetSequence(train_data_dir, PLANE, train_labels_acl, train_labels_meniscus,\n",
    "                          train_exam_ids, BATCH_SIZE, IMG_SIZE, is_train=True)\n",
    "\n",
    "valid_gen = MRNetSequence(valid_data_dir, PLANE, valid_labels_acl, valid_labels_meniscus,\n",
    "                          valid_exam_ids, BATCH_SIZE, IMG_SIZE, is_train=False) # is_train=False for validation (no shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9940bc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "\n",
      "Model Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Xception_MRNet_sagittal\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Xception_MRNet_sagittal\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ slice_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ xception            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │ slice_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ slice_gap           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ xception[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ head_dropout        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ slice_gap[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ acl_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │ head_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ meniscus_output     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │ head_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ slice_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m299\u001b[0m, \u001b[38;5;34m299\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ xception            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m,    │ \u001b[38;5;34m20,861,480\u001b[0m │ slice_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m2048\u001b[0m)             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ slice_gap           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ xception[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ head_dropout        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ slice_gap[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ acl_output (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m2,049\u001b[0m │ head_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ meniscus_output     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m2,049\u001b[0m │ head_dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,865,578</span> (79.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,865,578\u001b[0m (79.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,098</span> (16.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,098\u001b[0m (16.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up callbacks...\n",
      "\n",
      "--- Starting Training for sagittal plane ---\n",
      "Epochs: 50, Batch Size (Exams): 4, Learning Rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m  6/283\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31:54\u001b[0m 7s/step - acl_output_acc_acl: 0.4870 - acl_output_auc_acl: 0.4442 - acl_output_loss: 0.7112 - loss: 1.4769 - meniscus_output_acc_meniscus: 0.4204 - meniscus_output_auc_meniscus: 0.4783 - meniscus_output_loss: 0.7670"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck if data exists in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_data_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPLANE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_data_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPLANE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_gen,\n\u001b[0;32m     53\u001b[0m                         epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[0;32m     54\u001b[0m                         validation_data\u001b[38;5;241m=\u001b[39mvalid_gen,\n\u001b[0;32m     55\u001b[0m                         callbacks\u001b[38;5;241m=\u001b[39m[checkpoint, early_stopping, reduce_lr],\n\u001b[0;32m     56\u001b[0m                         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Set verbose=1 or 2 to see progress per epoch\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Finished ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# --- Plot Training History ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1694\u001b[0m   )\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Marc Velasquez\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Build Model ---\n",
    "print(\"\\nBuilding model...\")\n",
    "model = build_xception_model(input_shape=(*IMG_SIZE, N_CHANNELS),\n",
    "                             dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "# --- Compile Model ---\n",
    "# We have two outputs, so we use a dictionary for losses and metrics.\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss={'acl_output': BinaryCrossentropy(name='acl_loss'),\n",
    "                    'meniscus_output': BinaryCrossentropy(name='meniscus_loss')},\n",
    "              loss_weights={'acl_output': 1.0, 'meniscus_output': 1.0}, # Give equal importance to both tasks\n",
    "              metrics={'acl_output': [AUC(name='auc_acl'), BinaryAccuracy(name='acc_acl')],\n",
    "                       'meniscus_output': [AUC(name='auc_meniscus'), BinaryAccuracy(name='acc_meniscus')]})\n",
    "\n",
    "print(\"\\nModel Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# --- Callbacks ---\n",
    "print(\"\\nSetting up callbacks...\")\n",
    "# Save the best model based on validation loss (sum of both task losses)\n",
    "checkpoint = ModelCheckpoint(MODEL_SAVE_PATH,\n",
    "                             monitor='val_loss', # Monitor the total validation loss\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False, # Save the entire model structure + weights\n",
    "                             mode='min', # Minimize the validation loss\n",
    "                             verbose=1)\n",
    "\n",
    "# Stop training early if validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=10, # Stop after 10 epochs of no improvement in val_loss\n",
    "                               mode='min',\n",
    "                               restore_best_weights=True, # Load weights from the best epoch found\n",
    "                               verbose=1)\n",
    "\n",
    "# Reduce learning rate if validation loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2, # Reduce LR by a factor of 5 (lr = lr * 0.2)\n",
    "                              patience=5,  # Reduce LR after 5 epochs of no improvement\n",
    "                              mode='min',\n",
    "                              min_lr=1e-6, # Minimum learning rate\n",
    "                              verbose=1)\n",
    "\n",
    "# --- Train Model ---\n",
    "print(f\"\\n--- Starting Training for {PLANE} plane ---\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch Size (Exams): {BATCH_SIZE}, Learning Rate: {LEARNING_RATE}\")\n",
    "\n",
    "# Check if generators have data\n",
    "if len(train_gen) == 0 or len(valid_gen) == 0:\n",
    "     print(\"\\nError: Training or validation generator is empty. Cannot start training.\")\n",
    "     print(f\"Check if data exists in '{train_data_dir}/{PLANE}' and '{valid_data_dir}/{PLANE}'.\")\n",
    "else:\n",
    "    history = model.fit(train_gen,\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=valid_gen,\n",
    "                        callbacks=[checkpoint, early_stopping, reduce_lr],\n",
    "                        verbose=1) # Set verbose=1 or 2 to see progress per epoch\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "\n",
    "    # --- Plot Training History ---\n",
    "    if 'history' in locals() and history is not None:\n",
    "        print(\"\\nPlotting training history...\")\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        # Loss Plot\n",
    "        axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "        axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0].set_title(f'{PLANE} - Model Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss (BCE)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "\n",
    "        # AUC Plot (Average of ACL and Meniscus AUC)\n",
    "        # Note: Keras history keys might vary slightly based on TF version, check history.history.keys() if needed\n",
    "        train_auc_avg = (np.array(history.history.get('acl_output_auc_acl', history.history.get('auc_acl', 0))) +\n",
    "                         np.array(history.history.get('meniscus_output_auc_meniscus', history.history.get('auc_meniscus', 0)))) / 2.0\n",
    "        val_auc_avg = (np.array(history.history.get('val_acl_output_auc_acl', history.history.get('val_auc_acl', 0))) +\n",
    "                       np.array(history.history.get('val_meniscus_output_auc_meniscus', history.history.get('val_auc_meniscus', 0)))) / 2.0\n",
    "\n",
    "        axes[1].plot(train_auc_avg, label='Train Avg AUC')\n",
    "        axes[1].plot(val_auc_avg, label='Validation Avg AUC')\n",
    "        axes[1].set_title(f'{PLANE} - Model Average AUC')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Average AUC')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping history plots as training did not complete successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf161a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Evaluation ---\n",
    "print(f\"\\n--- Final Evaluation on Validation Set using Best Model ---\")\n",
    "print(f\"Loading best model weights from: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    # Load the model saved by ModelCheckpoint\n",
    "    best_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
    "    print(\"Best model loaded successfully.\")\n",
    "\n",
    "    # Evaluate using the aggregated evaluation function\n",
    "    validation_results = evaluate_model_aggregated(best_model,\n",
    "                                                   valid_data_dir,\n",
    "                                                   PLANE,\n",
    "                                                   valid_labels_acl,\n",
    "                                                   valid_labels_meniscus,\n",
    "                                                   valid_exam_ids,\n",
    "                                                   IMG_SIZE)\n",
    "\n",
    "    # Optional: Plot ROC Curves for the validation set\n",
    "    if validation_results:\n",
    "        print(\"\\nPlotting ROC Curves for Validation Set...\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        # ACL ROC\n",
    "        fpr_acl, tpr_acl, _ = roc_curve(validation_results['acl']['true'], validation_results['acl']['pred_prob'])\n",
    "        plt.plot(fpr_acl, tpr_acl, label=f\"ACL (AUC = {validation_results['acl']['auc']:.3f})\")\n",
    "\n",
    "        # Meniscus ROC\n",
    "        fpr_men, tpr_men, _ = roc_curve(validation_results['meniscus']['true'], validation_results['meniscus']['pred_prob'])\n",
    "        plt.plot(fpr_men, tpr_men, label=f\"Meniscus (AUC = {validation_results['meniscus']['auc']:.3f})\")\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Chance') # Diagonal line\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.title(f'ROC Curves - {PLANE} Plane - Validation Set')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Best model file not found at {MODEL_SAVE_PATH}. Evaluation skipped.\")\n",
    "    print(\"This might happen if training was interrupted or did not improve over initial weights.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
